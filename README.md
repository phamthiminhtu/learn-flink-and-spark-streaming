# Streaming with Flink & Spark

A repo to explore different streaming mechanisms: Kafka, Flink, and Spark.
The main goal is to understand the core components of Flink and Spark in streaming pipelines and how each serves a different purpose.

## Quick Start

### 1. Start services

```bash
docker compose up -d
```

What starts:
- Kafka (port 9092) - message broker
- Schema Registry (port 8081) - manages Avro schemas
- MinIO (ports 9000, 9001) - S3-compatible storage
- Kafka UI (port 8080) - web UI to browse topics/messages
- Producer container - has Python + dependencies installed

### 2. Generate test data

**Easy way** - run the onboarding script:

```bash
./shared/data-generator/start-producer.sh
```

It checks prerequisites, starts services, creates topics, and lets you pick how to run the producer.

**Manual way** - run the producer directly:

```bash
docker exec -it clickstream-producer python3 produce-test-events.py
```

### 3. Check if data is flowing

Open Kafka UI: http://localhost:8080

Or use command line:
```bash
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic clickstream \
  --from-beginning \
  --max-messages 10
```

## How it works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer   â”‚ â†’ Generates realistic clickstream events
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka     â”‚ â†’ Message broker
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â–º Flink  â†’ Real-time stream processing
       â”‚
       â””â”€â–º Spark  â†’ Batch processing & analytics
             â”‚
             â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  MinIO   â”‚ â†’ Data lake storage
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
streaming-with-flink-spark/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ flink-conf.yaml
â”‚   â””â”€â”€ spark-defaults.conf
â”œâ”€â”€ data/
â”‚   â””â”€â”€ schemas/                 # Avro schemas
â”‚       â”œâ”€â”€ clickstream-event.avsc
â”‚       â””â”€â”€ clickstream-event-readable.avsc
â”œâ”€â”€ docker-compose.yml           # Service definitions
â”œâ”€â”€ flink/                       # Flink jobs
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/
â”œâ”€â”€ kafka/                       # Data producer
â”‚   â”œâ”€â”€ produce-test-events.py  # Main producer script
â”‚   â””â”€â”€ README.md               # Producer documentation
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ data-generator/         # Onboarding & utilities
â”‚   â”‚   â”œâ”€â”€ start-producer.sh   # ğŸŒŸ New user script
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ QUICK_REFERENCE.md
â”‚   â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ tests/
â””â”€â”€ spark/                      # Spark jobs
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ src/
```

## What data gets generated

The producer simulates e-commerce user behavior:

- **Events**: VIEW, ADD_TO_CART, REMOVE_FROM_CART, CHECKOUT, PURCHASE
- **Rate**: 10 events/sec (you can change this)
- **Users**: 100 fake users with shopping sessions
- **Products**: 50 products in 5 categories
- **Late data**: 5% of events arrive 10-120 seconds late (good for testing watermarks)
- **Out-of-order**: Events don't always arrive in order (realistic scenario)

## Common commands

### Services

```bash
docker compose up -d              # Start everything
docker compose down               # Stop everything
docker compose ps                 # Check what's running
docker compose logs -f producer   # View logs
```

### Producer

```bash
# Start producer (see live output, Ctrl+C to stop)
docker exec -it clickstream-producer python3 produce-test-events.py

# Start in background
docker exec -d clickstream-producer python3 produce-test-events.py

# Stop producer
docker exec clickstream-producer pkill -f produce-test-events.py
```

### Kafka

```bash
# List topics
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Read messages from topic
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic clickstream \
  --from-beginning
```

### Jump into containers

```bash
docker exec -it clickstream-producer bash  # Producer
docker exec -it kafka bash                 # Kafka
```

## Web UIs

- Kafka UI: http://localhost:8080 - browse topics and messages
- MinIO Console: http://localhost:9001 - view stored files (login: admin/password123)
- Schema Registry: http://localhost:8081/subjects - view registered schemas

## More docs

- Producer details: `kafka/README.md`
- Data generator guide: `shared/data-generator/README.md`
- Command cheatsheet: `shared/data-generator/QUICK_REFERENCE.md`

## Common issues

**Services won't start**
```bash
docker ps                          # Is Docker running?
docker compose logs kafka          # Check for errors
docker compose restart kafka       # Restart if needed
```

**Producer timeout error**
```bash
# Check if Kafka is healthy (takes 30-60 sec to start)
docker inspect -f '{{.State.Health.Status}}' kafka

# Should show "healthy". If not, wait or restart:
docker compose restart kafka
```

**Topic doesn't exist**
```bash
# Create topics
docker compose up kafka-setup

# Check if they exist
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
```

## What to learn next

1. Start services âœ…
2. Generate test data âœ…
3. Build Flink streaming jobs (real-time processing)
4. Build Spark batch jobs (historical analysis)
5. Add monitoring (Prometheus/Grafana)
